{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42a651c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\", 'It goes like this', 'The fourth, the fifth', 'The minor fall, the major lift', 'The baffled king composing Hallelujah Hallelujah', 'Hallelujah', 'Hallelujah', 'Hallelujah Your faith was strong but you needed proof']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os, re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79c6ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d0ef124",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0:\n",
    "        continue\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    if len(preprocessed_sentence.split()) > 15:\n",
    "        continue\n",
    "    \n",
    "#     preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca66f1e",
   "metadata": {},
   "source": [
    "처음에 어떻게 해야 텐서가 15개를 넘어가는 것을 제외해야 될지 몰라서 고민하며 파이썬 문법을 보다 보니\n",
    "아주 간단하게 split() 메서드를 통해 분해할 수 있어서 사용했다\n",
    "\n",
    "근데 이게 맞는지는 모르겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27b43d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ea4d49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4 ...    0    0    0]\n",
      " [   2   15 2971 ...    0    0    0]\n",
      " [   2   33    7 ...   46    3    0]\n",
      " ...\n",
      " [   2    4  117 ...    0    0    0]\n",
      " [   2  258  195 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f762ff32850>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=20000, filters=' ', oov_token=\"<unknown>\")\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cccfd7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4   95  303   62   53    9  946 6269]\n",
      " [   2   15 2971  872    5    8   11 5747    6  374]\n",
      " [   2   33    7   40   16  164  288   28  333    5]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])\n",
    "print(type(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99c9f641",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source train : (124981, 14)\n",
      "Target train :  (124981, 14)\n"
     ]
    }
   ],
   "source": [
    "source_input = tensor[:, :-1]  \n",
    "target_input = tensor[:, 1:]    \n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(source_input, target_input, test_size=0.2, random_state=43)\n",
    "\n",
    "print(\"source train :\", enc_train.shape)\n",
    "print(\"Target train : \", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8884788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUFFER_SIZE = len(source_input)\n",
    "# BATCH_SIZE = 256\n",
    "# steps_per_epoch = len(source_input) // BATCH_SIZE\n",
    "\n",
    "# VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((source_input, target_input))\n",
    "# dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "# dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d4e24b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 1000\n",
    "hidden_size = 3000\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d48cb53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3906/3906 [==============================] - 1036s 253ms/step - loss: 2.9822 - val_loss: 2.6886\n",
      "Epoch 2/10\n",
      "3906/3906 [==============================] - 990s 254ms/step - loss: 2.3941 - val_loss: 2.4223\n",
      "Epoch 3/10\n",
      "3906/3906 [==============================] - 990s 254ms/step - loss: 1.9008 - val_loss: 2.2827\n",
      "Epoch 4/10\n",
      "3906/3906 [==============================] - 990s 253ms/step - loss: 1.5275 - val_loss: 2.2453\n",
      "Epoch 5/10\n",
      "3906/3906 [==============================] - 990s 253ms/step - loss: 1.2893 - val_loss: 2.2814\n",
      "Epoch 6/10\n",
      "3906/3906 [==============================] - 990s 253ms/step - loss: 1.1551 - val_loss: 2.3298\n",
      "Epoch 7/10\n",
      "3906/3906 [==============================] - 990s 253ms/step - loss: 1.0885 - val_loss: 2.3810\n",
      "Epoch 8/10\n",
      "3906/3906 [==============================] - 989s 253ms/step - loss: 1.0506 - val_loss: 2.4266\n",
      "Epoch 9/10\n",
      "3906/3906 [==============================] - 989s 253ms/step - loss: 1.0289 - val_loss: 2.4446\n",
      "Epoch 10/10\n",
      "3906/3906 [==============================] - 989s 253ms/step - loss: 1.0134 - val_loss: 2.4870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f76244202e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(enc_train, dec_train, epochs=10, validation_data=(enc_val, dec_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c9249a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "    \n",
    "    while True:\n",
    "        predict = model(test_tensor)\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        if predict_word.numpy()[0] == end_token:\n",
    "            break\n",
    "        if test_tensor.shape[1] >= max_len: \n",
    "            break\n",
    "\n",
    "    generated = \"\"\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cd67e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , liberian girl , <end> '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd31286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
